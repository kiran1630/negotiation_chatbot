{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GROQ_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0, groq_api_key=os.getenv(\"GROQ_API_KEY\"), model_name=\"llama-3.1-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_extract = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            ###  your are interactive chatbot and answer the question \n",
    "            ### great and answer the question \n",
    "            \"\"\"\n",
    "        )\n",
    "chain_extract = llm\n",
    "# res = chain_extract.invoke(input={'hi hello could you give me this item for 100 rupe'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = chain_extract.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob, it's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = llm.invoke(input_messages, config)\n",
    "print(output.content)  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms my name?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m [HumanMessage(\u001b[38;5;28mstr\u001b[39m(query))]\n\u001b[1;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mchain_extract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32me:\\users\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:285\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m--> 285\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[0;32m    286\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    287\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    288\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    289\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    292\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    293\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32me:\\users\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:268\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mconvert_to_messages(\u001b[38;5;28minput\u001b[39m))\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    271\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any information about your name. This is the beginning of our conversation, and I don't have any prior knowledge about you. If you'd like to share your name, I'd be happy to chat with you.\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi ! what is my name.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = llm.invoke(input_messages, config)\n",
    "print(output.content)  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of items with original and selling prices\n",
    "items = [\n",
    "    {\"Item\": \"Smartphone\", \"Original Price\": 500, \"Selling Price\": 550},\n",
    "    {\"Item\": \"Laptop\", \"Original Price\": 800, \"Selling Price\": 880},\n",
    "    {\"Item\": \"Headphones\", \"Original Price\": 50, \"Selling Price\": 60},\n",
    "    {\"Item\": \"Smartwatch\", \"Original Price\": 150, \"Selling Price\": 165},\n",
    "    {\"Item\": \"Bluetooth Speaker\", \"Original Price\": 100, \"Selling Price\": 120},\n",
    "    {\"Item\": \"Tablet\", \"Original Price\": 300, \"Selling Price\": 350},\n",
    "    {\"Item\": \"Camera\", \"Original Price\": 600, \"Selling Price\": 680},\n",
    "    {\"Item\": \"Gaming Console\", \"Original Price\": 400, \"Selling Price\": 450},\n",
    "    {\"Item\": \"Wireless Mouse\", \"Original Price\": 20, \"Selling Price\": 25},\n",
    "    {\"Item\": \"Keyboard\", \"Original Price\": 30, \"Selling Price\": 40},\n",
    "    {\"Item\": \"Monitor\", \"Original Price\": 200, \"Selling Price\": 230},\n",
    "    {\"Item\": \"Printer\", \"Original Price\": 150, \"Selling Price\": 175},\n",
    "    {\"Item\": \"External Hard Drive\", \"Original Price\": 80, \"Selling Price\": 95},\n",
    "    {\"Item\": \"Router\", \"Original Price\": 70, \"Selling Price\": 85},\n",
    "    {\"Item\": \"Television\", \"Original Price\": 900, \"Selling Price\": 1000},\n",
    "    {\"Item\": \"Air Conditioner\", \"Original Price\": 1200, \"Selling Price\": 1300},\n",
    "    {\"Item\": \"Microwave Oven\", \"Original Price\": 250, \"Selling Price\": 275},\n",
    "    {\"Item\": \"Refrigerator\", \"Original Price\": 1000, \"Selling Price\": 1100},\n",
    "    {\"Item\": \"Washing Machine\", \"Original Price\": 700, \"Selling Price\": 780},\n",
    "    {\"Item\": \"Vacuum Cleaner\", \"Original Price\": 150, \"Selling Price\": 175}\n",
    "]\n",
    "\n",
    "# Create a pandas DataFrame from the list\n",
    "df = pd.DataFrame(items)\n",
    "\n",
    "# Output the DataFrame to a CSV file\n",
    "df.to_csv(\"ecommerce_items.csv\", index=False)\n",
    "\n",
    "print(\"CSV file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------+\n",
      "| Item                |   Selling Price |\n",
      "+=====================+=================+\n",
      "| Smartphone          |             550 |\n",
      "+---------------------+-----------------+\n",
      "| Laptop              |             880 |\n",
      "+---------------------+-----------------+\n",
      "| Headphones          |              60 |\n",
      "+---------------------+-----------------+\n",
      "| Smartwatch          |             165 |\n",
      "+---------------------+-----------------+\n",
      "| Bluetooth Speaker   |             120 |\n",
      "+---------------------+-----------------+\n",
      "| Tablet              |             350 |\n",
      "+---------------------+-----------------+\n",
      "| Camera              |             680 |\n",
      "+---------------------+-----------------+\n",
      "| Gaming Console      |             450 |\n",
      "+---------------------+-----------------+\n",
      "| Wireless Mouse      |              25 |\n",
      "+---------------------+-----------------+\n",
      "| Keyboard            |              40 |\n",
      "+---------------------+-----------------+\n",
      "| Monitor             |             230 |\n",
      "+---------------------+-----------------+\n",
      "| Printer             |             175 |\n",
      "+---------------------+-----------------+\n",
      "| External Hard Drive |              95 |\n",
      "+---------------------+-----------------+\n",
      "| Router              |              85 |\n",
      "+---------------------+-----------------+\n",
      "| Television          |            1000 |\n",
      "+---------------------+-----------------+\n",
      "| Air Conditioner     |            1300 |\n",
      "+---------------------+-----------------+\n",
      "| Microwave Oven      |             275 |\n",
      "+---------------------+-----------------+\n",
      "| Refrigerator        |            1100 |\n",
      "+---------------------+-----------------+\n",
      "| Washing Machine     |             780 |\n",
      "+---------------------+-----------------+\n",
      "| Vacuum Cleaner      |             175 |\n",
      "+---------------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "# display_items.py\n",
    "\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"ecommerce_items.csv\")\n",
    "\n",
    "# Select only 'Item' and 'Selling Price' columns\n",
    "data = df[['Item', 'Selling Price']][10:]\n",
    "\n",
    "# Display the data in a nice tabular format\n",
    "print(tabulate(data, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
